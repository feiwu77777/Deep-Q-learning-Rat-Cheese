{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fei/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 13\n",
    "T = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, n_action = 4, memory_size=100, batch_size = 16, n_state=2):\n",
    "        self.n_action = n_action\n",
    "        self.epsilon = epsilon\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        self.grid_size = grid_size\n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "        # Memory\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "    \n",
    "    def act(self,s):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "        else:\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def learned_act(self, s):\n",
    "        return np.argmax(self.model.predict(s[None])[0])\n",
    "\n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    "        self.memory.append([s_, n_s_, a_, r_, game_over_])\n",
    "        input_states = np.zeros((self.batch_size, 5, 5, self.n_state))\n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "        \n",
    "        replays = np.random.choice(np.arange(len(self.memory)), \n",
    "                                   size = self.batch_size, \n",
    "                                   replace = False)\n",
    "        \n",
    "        for i, n in enumerate(replays):\n",
    "            s_, n_s_, a_, r_, game_over_ = self.memory[n]\n",
    "            input_states[i] = s_\n",
    "            if game_over_: target_q[i, a_] = r_\n",
    "            else:\n",
    "                pred = self.model.predict(n_s_[None])\n",
    "                target_q[i, a_] = r_ + self.discount*np.max(pred)\n",
    "                \n",
    "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
    "        target_q = np.clip(target_q, -3, 3)\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        ###### FILL IN\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size = 2, activation = 'relu', input_shape = (5,5,self.n_state)))\n",
    "        model.add(Conv2D(64, kernel_size = 2, activation = 'relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4))\n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(agent, env):\n",
    "    # Render the environment\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    for i in range(agent.batch_size):\n",
    "        action = np.random.randint(0,4)\n",
    "\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        agent.memory.append([prev_state, state, action, reward, game_over])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, epochs, prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        state = env.reset()\n",
    "        # This assumes that the games will end\n",
    "        game_over = False\n",
    "\n",
    "        win = lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action, train = False)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0: win = win + reward\n",
    "            if reward < 0: lose = lose - reward\n",
    "        \n",
    "        # Save as a mp4\n",
    "        env.draw('video/test/' + prefix + str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epoch, prefix='', e_start = 1.0, e_end = 0.01, decay_rate = 0.001):\n",
    "    score = loss = 0\n",
    "    decay_step = 0\n",
    "    \n",
    "    init_memory(agent, env)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "        win = lose = 0\n",
    "        \n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            decay_step += 1\n",
    "            epsilon = e_end + (e_start - e_end) * np.exp(-decay_rate * decay_step)\n",
    "            agent.set_epsilon(epsilon)\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0: win += reward\n",
    "            if reward < 0: lose -= reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0: env.draw('video/train/' + prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights='model/' + prefix + '_model.h5',name_model='model/' + prefix + '_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentExploring(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=100\n",
    "        b[:,-2:,:]=100\n",
    "        b[:2,:,:]=100\n",
    "        b[:,:2,:]=100\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action, train = True):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        if train:\n",
    "            reward = self.malus_position[self.x, self.y] + self.board[self.x, self.y]\n",
    "        else:\n",
    "            reward = self.board[self.x, self.y]\n",
    "            \n",
    "        self.board[self.x, self.y] = 0\n",
    "        self.malus_position[self.x, self.y] = -0.1\n",
    "\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        self.malus_position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.malus_position[self.x, self.y] = -0.1\n",
    "\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/050 | Loss 0.0057 | Win/lose count 6.0/21.40000000000003 (-15.40000000000003)\n",
      "Epoch 001/050 | Loss 0.0101 | Win/lose count 8.0/30.500000000000103 (-22.500000000000103)\n",
      "Epoch 002/050 | Loss 0.0039 | Win/lose count 11.5/23.100000000000026 (-11.600000000000026)\n",
      "Epoch 003/050 | Loss 0.0103 | Win/lose count 9.0/20.200000000000024 (-11.200000000000024)\n",
      "Epoch 004/050 | Loss 0.0052 | Win/lose count 10.0/22.300000000000065 (-12.300000000000065)\n",
      "Epoch 005/050 | Loss 0.0039 | Win/lose count 14.0/20.6 (-6.600000000000001)\n",
      "Epoch 006/050 | Loss 0.0538 | Win/lose count 20.5/17.29999999999999 (3.20000000000001)\n",
      "Epoch 007/050 | Loss 0.0169 | Win/lose count 14.5/20.50000000000003 (-6.000000000000028)\n",
      "Epoch 008/050 | Loss 0.0059 | Win/lose count 20.5/16.499999999999975 (4.000000000000025)\n",
      "Epoch 009/050 | Loss 0.0054 | Win/lose count 15.5/14.799999999999967 (0.700000000000033)\n",
      "Epoch 010/050 | Loss 0.0423 | Win/lose count 20.0/16.299999999999972 (3.7000000000000277)\n",
      "Epoch 011/050 | Loss 0.0535 | Win/lose count 18.5/17.39999999999999 (1.1000000000000085)\n",
      "Epoch 012/050 | Loss 0.0077 | Win/lose count 22.5/11.799999999999974 (10.700000000000026)\n",
      "Epoch 013/050 | Loss 0.0454 | Win/lose count 14.0/16.199999999999964 (-2.1999999999999638)\n",
      "Epoch 014/050 | Loss 0.0160 | Win/lose count 14.0/16.29999999999996 (-2.2999999999999616)\n",
      "Epoch 015/050 | Loss 0.0095 | Win/lose count 19.5/16.399999999999963 (3.100000000000037)\n",
      "Epoch 016/050 | Loss 0.0427 | Win/lose count 12.5/16.99999999999997 (-4.499999999999972)\n",
      "Epoch 017/050 | Loss 0.0213 | Win/lose count 19.0/17.999999999999993 (1.000000000000007)\n",
      "Epoch 018/050 | Loss 0.0059 | Win/lose count 6.0/16.89999999999997 (-10.89999999999997)\n",
      "Epoch 019/050 | Loss 0.0057 | Win/lose count 19.5/13.69999999999997 (5.800000000000029)\n",
      "Epoch 020/050 | Loss 0.0077 | Win/lose count 11.5/16.299999999999965 (-4.799999999999965)\n",
      "Epoch 021/050 | Loss 0.0060 | Win/lose count 11.0/16.999999999999975 (-5.999999999999975)\n",
      "Epoch 022/050 | Loss 0.0043 | Win/lose count 22.5/11.599999999999975 (10.900000000000025)\n",
      "Epoch 023/050 | Loss 0.0070 | Win/lose count 13.0/14.899999999999967 (-1.8999999999999666)\n",
      "Epoch 024/050 | Loss 0.0057 | Win/lose count 11.0/14.699999999999964 (-3.6999999999999638)\n",
      "Epoch 025/050 | Loss 0.0058 | Win/lose count 19.5/12.099999999999973 (7.400000000000027)\n",
      "Epoch 026/050 | Loss 0.0063 | Win/lose count 13.5/13.499999999999968 (3.197442310920451e-14)\n",
      "Epoch 027/050 | Loss 0.0061 | Win/lose count 20.0/14.699999999999964 (5.300000000000036)\n",
      "Epoch 028/050 | Loss 0.0101 | Win/lose count 18.5/13.899999999999967 (4.600000000000033)\n",
      "Epoch 029/050 | Loss 0.0113 | Win/lose count 14.5/15.499999999999968 (-0.999999999999968)\n",
      "Epoch 030/050 | Loss 0.0058 | Win/lose count 25.0/11.49999999999998 (13.50000000000002)\n",
      "Epoch 031/050 | Loss 0.0078 | Win/lose count 16.5/14.29999999999997 (2.2000000000000295)\n",
      "Epoch 032/050 | Loss 0.0057 | Win/lose count 20.5/13.299999999999972 (7.200000000000028)\n",
      "Epoch 033/050 | Loss 0.0118 | Win/lose count 10.5/17.49999999999998 (-6.999999999999979)\n",
      "Epoch 034/050 | Loss 0.0086 | Win/lose count 7.5/16.09999999999996 (-8.599999999999959)\n",
      "Epoch 035/050 | Loss 0.0070 | Win/lose count 13.0/12.599999999999971 (0.4000000000000288)\n",
      "Epoch 036/050 | Loss 0.0148 | Win/lose count 19.0/15.199999999999962 (3.800000000000038)\n",
      "Epoch 037/050 | Loss 0.0095 | Win/lose count 25.0/11.199999999999976 (13.800000000000024)\n",
      "Epoch 038/050 | Loss 0.0169 | Win/lose count 12.5/15.599999999999968 (-3.0999999999999677)\n",
      "Epoch 039/050 | Loss 0.0137 | Win/lose count 13.0/14.499999999999964 (-1.4999999999999645)\n",
      "Epoch 040/050 | Loss 0.0108 | Win/lose count 13.5/14.999999999999963 (-1.4999999999999627)\n",
      "Epoch 041/050 | Loss 0.0107 | Win/lose count 7.0/17.599999999999984 (-10.599999999999984)\n",
      "Epoch 042/050 | Loss 0.0137 | Win/lose count 6.5/18.09999999999999 (-11.59999999999999)\n",
      "Epoch 043/050 | Loss 0.0195 | Win/lose count 24.0/12.399999999999975 (11.600000000000025)\n",
      "Epoch 044/050 | Loss 0.0092 | Win/lose count 17.5/14.499999999999968 (3.000000000000032)\n",
      "Epoch 045/050 | Loss 0.0170 | Win/lose count 12.0/17.099999999999973 (-5.099999999999973)\n",
      "Epoch 046/050 | Loss 0.0174 | Win/lose count 22.0/13.099999999999978 (8.900000000000022)\n",
      "Epoch 047/050 | Loss 0.0086 | Win/lose count 20.5/14.99999999999997 (5.50000000000003)\n",
      "Epoch 048/050 | Loss 0.0127 | Win/lose count 17.0/14.699999999999976 (2.300000000000024)\n",
      "Epoch 049/050 | Loss 0.0462 | Win/lose count 17.0/11.799999999999974 (5.200000000000026)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train(agent, env, 50, prefix='explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of the CNN\n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32, n_state = 3)\n",
    "agent.load(name_weights='model/model.h5',name_model='model/model.json')\n",
    "print('Test of the CNN')\n",
    "#test(agent,env,5,prefix='explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.5 45\n",
      "24.0 570\n"
     ]
    }
   ],
   "source": [
    "prefix = 'win3'\n",
    "c = 0\n",
    "max_score = 0\n",
    "min_score = 100\n",
    "scores = []\n",
    "while c < 1000:\n",
    "\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = lose = 0\n",
    "\n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action, train = False)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0: win = win + reward\n",
    "        if reward < 0: lose = lose -reward\n",
    "            \n",
    "    c += 1\n",
    "    scores.append(score)\n",
    "    if 0.5 not in env.board[2:-2,2:-2] and win-lose>score:\n",
    "        score = win-lose\n",
    "        print(score, c)\n",
    "        env.draw('video/test/' + prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
